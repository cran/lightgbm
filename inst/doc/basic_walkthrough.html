<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>Basic Walkthrough</title>
<style type="text/css">
/**
 * Prism.s theme ported from highlight.js's xcode style
 */
pre code {
  padding: 1em;
}
.token.comment {
  color: #007400;
}
.token.punctuation {
  color: #999;
}
.token.tag,
.token.selector {
  color: #aa0d91;
}
.token.boolean,
.token.number,
.token.constant,
.token.symbol {
  color: #1c00cf;
}
.token.property,
.token.attr-name,
.token.string,
.token.char,
.token.builtin {
  color: #c41a16;
}
.token.inserted {
  background-color: #ccffd8;
}
.token.deleted {
  background-color: #ffebe9;
}
.token.operator,
.token.entity,
.token.url,
.language-css .token.string,
.style .token.string {
  color: #9a6e3a;
}
.token.atrule,
.token.attr-value,
.token.keyword {
  color: #836c28;
}
.token.function,
.token.class-name {
  color: #DD4A68;
}
.token.regex,
.token.important,
.token.variable {
  color: #5c2699;
}
.token.important,
.token.bold {
  font-weight: bold;
}
.token.italic {
  font-style: italic;
}
</style>
<style type="text/css">
body {
  font-family: sans-serif;
  max-width: 800px;
  margin: auto;
  padding: 1em;
  line-height: 1.5;
  box-sizing: border-box;
}
body, .footnotes, code { font-size: .9em; }
li li { font-size: .95em; }
*, *:before, *:after {
  box-sizing: inherit;
}
pre, img { max-width: 100%; }
pre, pre:hover {
  white-space: pre-wrap;
  word-break: break-all;
}
pre code {
  display: block;
  overflow-x: auto;
}
code { font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace; }
:not(pre) > code, code[class] { background-color: #F8F8F8; }
code.language-undefined, pre > code:not([class]) {
  background-color: inherit;
  border: 1px solid #eee;
}
table {
  margin: auto;
  border-top: 1px solid #666;
}
table thead th { border-bottom: 1px solid #ddd; }
th, td { padding: 5px; }
thead, tfoot, tr:nth-child(even) { background: #eee; }
blockquote {
  color: #666;
  margin: 0;
  padding-left: 1em;
  border-left: 0.5em solid #eee;
}
hr, .footnotes::before { border: 1px dashed #ddd; }
.frontmatter { text-align: center; }
#TOC .numbered li { list-style: none; }
#TOC .numbered { padding-left: 0; }
#TOC .numbered ul { padding-left: 1em; }
table, .body h2 { border-bottom: 1px solid #666; }
.body .appendix, .appendix ~ h2 { border-bottom-style: dashed; }
.footnote-ref a::before { content: "["; }
.footnote-ref a::after { content: "]"; }
section.footnotes::before {
  content: "";
  display: block;
  max-width: 20em;
}

@media print {
  body {
    font-size: 12pt;
    max-width: 100%;
  }
  tr, img { page-break-inside: avoid; }
}
@media only screen and (min-width: 992px) {
  pre { white-space: pre; }
}
</style>
</head>
<body>
<div class="frontmatter">
<div class="title"><h1>Basic Walkthrough</h1></div>
<div class="author"><h2></h2></div>
<div class="date"><h3></h3></div>
</div>
<div class="body">
<div id="TOC">
<ul class="numbered">
<li><a href="#introduction"><span class="section-number">1.</span> Introduction</a></li>
<li><a href="#the-dataset"><span class="section-number">2.</span> The dataset</a></li>
<li><a href="#training-the-model"><span class="section-number">3.</span> Training the model</a>
<ul>
<li><a href="#using-the-lightgbm-function"><span class="section-number">3.1</span> Using the <code>lightgbm()</code> function</a></li>
<li><a href="#using-the-lgb-train-function"><span class="section-number">3.2</span> Using the <code>lgb.train()</code> function</a></li>
</ul>
</li>
<li><a href="#references"><span class="section-number">4.</span> References</a></li>
</ul>
</div>
<h2 id="introduction"><span class="section-number">1.</span> Introduction</h2>
<p>Welcome to the world of <a href="https://lightgbm.readthedocs.io/en/latest/">LightGBM</a>, a highly efficient gradient boosting implementation (Ke et al. 2017).</p>
<pre><code class="language-r">library(lightgbm)
</code></pre>
<p>This vignette will guide you through its basic usage. It will show how to build a simple binary classification model based on a subset of the <code>bank</code> dataset (Moro, Cortez, and Rita 2014). You will use the two input features “age” and “balance” to predict whether a client has subscribed a term deposit.</p>
<h2 id="the-dataset"><span class="section-number">2.</span> The dataset</h2>
<p>The dataset looks as follows.</p>
<pre><code class="language-r">data(bank, package = &quot;lightgbm&quot;)

bank[1L:5L, c(&quot;y&quot;, &quot;age&quot;, &quot;balance&quot;)]
#&gt;         y   age balance
#&gt;    &lt;char&gt; &lt;int&gt;   &lt;int&gt;
#&gt; 1:     no    30    1787
#&gt; 2:     no    33    4789
#&gt; 3:     no    35    1350
#&gt; 4:     no    30    1476
#&gt; 5:     no    59       0

# Distribution of the response
table(bank$y)
#&gt; 
#&gt;   no  yes 
#&gt; 4000  521
</code></pre>
<h2 id="training-the-model"><span class="section-number">3.</span> Training the model</h2>
<p>The R-package of LightGBM offers two functions to train a model:</p>
<ul>
<li><code>lgb.train()</code>: This is the main training logic. It offers full flexibility but requires a <code>Dataset</code> object created by the <code>lgb.Dataset()</code> function.</li>
<li><code>lightgbm()</code>: Simpler, but less flexible. Data can be passed without having to bother with <code>lgb.Dataset()</code>.</li>
</ul>
<h3 id="using-the-lightgbm-function"><span class="section-number">3.1</span> Using the <code>lightgbm()</code> function</h3>
<p>In a first step, you need to convert data to numeric. Afterwards, you are ready to fit the model by the <code>lightgbm()</code> function.</p>
<pre><code class="language-r"># Numeric response and feature matrix
y &lt;- as.numeric(bank$y == &quot;yes&quot;)
X &lt;- data.matrix(bank[, c(&quot;age&quot;, &quot;balance&quot;)])

# Train
fit &lt;- lightgbm(
  data = X
  , label = y
  , params = list(
    num_leaves = 4L
    , learning_rate = 1.0
    , objective = &quot;binary&quot;
  )
  , nrounds = 10L
  , verbose = -1L
)

# Result
summary(predict(fit, X))
#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
#&gt; 0.01192 0.07370 0.09871 0.11593 0.14135 0.65796
</code></pre>
<p>It seems to have worked! And the predictions are indeed probabilities between 0 and 1.</p>
<h3 id="using-the-lgb-train-function"><span class="section-number">3.2</span> Using the <code>lgb.train()</code> function</h3>
<p>Alternatively, you can go for the more flexible interface <code>lgb.train()</code>. Here, as an additional step, you need to prepare <code>y</code> and <code>X</code> by the data API <code>lgb.Dataset()</code> of LightGBM. Parameters are passed to <code>lgb.train()</code> as a named list.</p>
<pre><code class="language-r"># Data interface
dtrain &lt;- lgb.Dataset(X, label = y)

# Parameters
params &lt;- list(
  objective = &quot;binary&quot;
  , num_leaves = 4L
  , learning_rate = 1.0
)

# Train
fit &lt;- lgb.train(
  params
  , data = dtrain
  , nrounds = 10L
  , verbose = -1L
)
</code></pre>
<p>Try it out! If stuck, visit LightGBM’s <a href="https://lightgbm.readthedocs.io/en/latest/R/index.html">documentation</a> for more details.</p>
<h2 id="references"><span class="section-number">4.</span> References</h2>
<p>Ke, Guolin, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. 2017. “LightGBM: A Highly Efficient Gradient Boosting Decision Tree.” In Advances in Neural Information Processing Systems 30 (NIPS 2017).</p>
<p>Moro, Sérgio, Paulo Cortez, and Paulo Rita. 2014. “A Data-Driven Approach to Predict the Success of Bank Telemarketing.” Decision Support Systems 62: 22–31.</p>
</div>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-core.min.js" defer></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/autoloader/prism-autoloader.min.js" defer></script>
</body>
</html>
